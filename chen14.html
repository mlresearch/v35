<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>A Convex Formulation for Mixed Regression with Two Components: Minimax Optimal Rates | COLT 2014 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="A Convex Formulation for Mixed Regression with Two Components: Minimax Optimal Rates">

  <meta name="citation_author" content="Chen, Yudong">

  <meta name="citation_author" content="Yi, Xinyang">

  <meta name="citation_author" content="Caramanis, Constantine">

<meta name="citation_publication_date" content="2014">
<meta name="citation_conference_title" content="Proceedings of The 27th Conference on Learning Theory">
<meta name="citation_firstpage" content="560">
<meta name="citation_lastpage" content="604">
<meta name="citation_pdf_url" content="chen14.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>A Convex Formulation for Mixed Regression with Two Components: Minimax Optimal Rates</h1>

	<div id="authors">
	
		Yudong Chen,
	
		Xinyang Yi,
	
		Constantine Caramanis
	</div>;
	<div id="info">
		JMLR W&amp;CP 35 
		
		: 
		560â€“604, 2014
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		We consider the mixed regression problem with two components, under adversarial and stochastic noise. We give a convex optimization formulation that provably recovers the true solution, and provide upper bounds on the recovery errors for both arbitrary noise and stochastic noise settings. We also give matching minimax lower bounds (up to log factors), showing that under certain assumptions, our algorithm is information-theoretically optimal. Our results represent the first (and currently only known) tractable algorithm guaranteeing successful recovery with tight bounds on recovery errors and sample complexity.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="chen14.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
