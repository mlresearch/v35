<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Robust Multi-objective Learning with Mentor Feedback | COLT 2014 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Robust Multi-objective Learning with Mentor Feedback">

  <meta name="citation_author" content="Agarwal, Alekh">

  <meta name="citation_author" content="Badanidiyuru, Ashwinkumar">

  <meta name="citation_author" content="Dudík, Miroslav">

  <meta name="citation_author" content="Schapire, Robert E.">

  <meta name="citation_author" content="Slivkins, Aleksandrs">

<meta name="citation_publication_date" content="2014">
<meta name="citation_conference_title" content="Proceedings of The 27th Conference on Learning Theory">
<meta name="citation_firstpage" content="726">
<meta name="citation_lastpage" content="741">
<meta name="citation_pdf_url" content="agarwal14b.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Robust Multi-objective Learning with Mentor Feedback</h1>

	<div id="authors">
	
		Alekh Agarwal,
	
		Ashwinkumar Badanidiyuru,
	
		Miroslav Dudík,
	
		Robert E. Schapire,
	
		Aleksandrs Slivkins
	</div>;
	<div id="info">
		JMLR W&amp;CP 35 
		
		: 
		726–741, 2014
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		We study decision making when each action is described by a set of objectives, all of which are to be maximized. During the training phase, we have access to the actions of an outside agent (“mentor”). In the test phase, our goal is to maximally improve upon the mentor’s (unobserved) actions across all objectives. We present an algorithm with a vanishing regret compared with the optimal possible improvement, and show that our regret bound is the best possible. The bound is independent of the number of actions, and scales only as the logarithm of the number of objectives.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="agarwal14b.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
