<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>The sample complexity of agnostic learning under deterministic labels | COLT 2014 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="The sample complexity of agnostic learning under deterministic labels">

  <meta name="citation_author" content="Ben-David, Shai">

  <meta name="citation_author" content="Urner, Ruth">

<meta name="citation_publication_date" content="2014">
<meta name="citation_conference_title" content="Proceedings of The 27th Conference on Learning Theory">
<meta name="citation_firstpage" content="527">
<meta name="citation_lastpage" content="542">
<meta name="citation_pdf_url" content="ben-david14.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>The sample complexity of agnostic learning under deterministic labels</h1>

	<div id="authors">
	
		Shai Ben-David,
	
		Ruth Urner
	</div>;
	<div id="info">
		JMLR W&amp;CP 35 
		
		: 
		527â€“542, 2014
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		With the emergence of Machine Learning tools that allow handling data with a huge number of features, it becomes reasonable to assume that, over the full set of features, the true labeling is (almost) fully determined. That is, the labeling function is deterministic, but not necessarily a member of some known hypothesis class. However, agnostic learning of deterministic labels has so far received little research attention. We investigate this setting and show that it displays a behavior that is quite different from that of the fundamental results of the common (PAC) learning setups. First, we show that the sample complexity of learning a binary hypothesis class (with respect to deterministic labeling functions) is not fully determined by the VC-dimension of the class. For any <span class="math">\(d\)</span>, we present classes of VC-dimension <span class="math">\(d\)</span> that are learnable from <span class="math">\({\tilde O}(d/\epsilon)\)</span>-many samples and classes that require samples of size <span class="math">\(\Omega(d/\epsilon^2)\)</span>. Furthermore, we show that in this setup, there are classes for which any proper learner has suboptimal sample complexity. While the class can be learned with sample complexity <span class="math">\({\tilde O}(d/\epsilon)\)</span>, any <em>proper</em> (and therefore, any ERM) algorithm requires <span class="math">\(\Omega(d/\epsilon^2)\)</span> samples. We provide combinatorial characterizations of both phenomena, and further analyze the utility of unlabeled samples in this setting. Lastly, we discuss the error rates of nearest neighbor algorithms under deterministic labels and additional niceness-of-data assumptions.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="ben-david14.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
