---
title: 'Logistic Regression: Tight Bounds for Stochastic and Online Optimization'
abstract: The logistic loss function is often advocated in machine learning and statistics
  as a smooth and strictly convex surrogate for the 0-1 loss. In this paper we investigate
  the question of whether these smoothness and convexity properties make the logistic
  loss preferable to other widely considered options such as the hinge loss. We show
  that in contrast to known asymptotic bounds, as long as the number of prediction/optimization
  iterations is sub exponential, the logistic loss provides no improvement over a
  generic non-smooth loss function such as the hinge loss. In particular we show that
  the convergence rate of stochastic logistic optimization is bounded from below by
  a polynomial in the diameter of the decision set and the number of prediction iterations,
  and provide a matching tight upper bound. This resolves the COLT open problem of
  McMahan and Streeter (2012).
layout: inproceedings
id: hazan14a
month: 0
firstpage: 197
lastpage: 209
page: 197-209
sections: 
author:
- given: Elad
  family: Hazan
- given: Tomer
  family: Koren
- given: Kfir Y.
  family: Levy
date: 2014-05-29
address: Barcelona, Spain
publisher: PMLR
container-title: Proceedings of The 27th Conference on Learning Theory
volume: '35'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 5
  - 29
pdf: http://proceedings.mlr.press/v35/hazan14a/hazan14a.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
