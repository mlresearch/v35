---
title: 'Open Problem: Online Local Learning'
abstract: 'In many learning problems, we attempt to infer \emphglobal structure in
  the interest of making \emphlocal predictions.  For example, we might try to infer
  the skills of the competitors in a tournament in order to predict who will win a
  match, or we might try to predict characteristics of users and films in order to
  predict which users will like which films.  In even relatively simple settings of
  this type, it is typically NP-hard to find the latent data which best explain some
  observations.  But do these complexity-theoretic obstructions actually prevent us
  from making good predictions?  Because each prediction depends on only a small number
  of variables, it might be possible to make good predictions without actually finding
  a good global assignment.  This may seem to be a purely technical distinction, but
  recent work has shown that several local prediction problems actually \emphare easy
  even though the corresponding global inference problem is hard.  The question we
  pose is: how general is this phenomenon?'
section: open
layout: inproceedings
series: Proceedings of Machine Learning Research
id: christiano14
month: 0
tex_title: 'Open Problem: Online Local Learning'
firstpage: 1290
lastpage: 1294
page: 1290-1294
sections: 
author:
- given: Paul
  family: Christiano
date: 2014-05-29
address: Barcelona, Spain
publisher: PMLR
container-title: Proceedings of The 27th Conference on Learning Theory
volume: '35'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 5
  - 29
pdf: http://proceedings.mlr.press/v35/christiano14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
