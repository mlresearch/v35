---
title: Stochastic Regret Minimization via Thompson Sampling
abstract: The Thompson Sampling (TS) policy  is a widely implemented algorithm for
  the stochastic multi-armed bandit (MAB) problem. Given a prior distribution over
  possible parameter settings of the underlying reward distributions of the arms,
  at each time instant, the policy plays an arm with probability equal to the probability
  that this arm has largest mean reward conditioned on the current posterior distributions
  of the arms.  This policy generalizes the celebrated “probability matching” heuristic
  which has been experimentally and widely observed in human decision making. However,
  despite its ubiquity, the Thompson Sampling policy is poorly understood. Our goal
  in this paper is to make progress towards understanding the empirical success of
  this policy. We proceed using the lens of approximation algorithms and problem definitions
  from stochastic optimization. We focus on an objective function termed \em stochastic
  regret that captures the expected number of times the policy plays an arm that is
  not the eventual best arm, where the expectation is over the prior distribution.
  Given such a definition, we show that TS is a 2–approximation to the optimal decision
  policy in two extreme but canonical scenarios. One such scenario is the two-armed
  bandit problem which is used as a calibration point in all bandit literature. The
  second scenario is stochastic optimization where the outcome of a random variable
  is revealed in a single play to a high or low deterministic value. We show that
  the 2 approximation is tight in both these scenarios. We provide an uniform analysis
  framework that in theory is capable of proving our  conjecture that the TS policy
  is a 2–approximation to the optimal decision policy for minimizing stochastic regret,
  for any prior distribution and any time horizon.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: guha14
month: 0
firstpage: 317
lastpage: 338
page: 317-338
sections: 
author:
- given: Sudipto
  family: Guha
- given: Kamesh
  family: Munagala
date: 2014-05-29
address: Barcelona, Spain
publisher: PMLR
container-title: Proceedings of The 27th Conference on Learning Theory
volume: '35'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 5
  - 29
pdf: http://proceedings.mlr.press/v35/guha14/guha14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
