---
title: Optimal learners for multiclass problems
abstract: 'The fundamental theorem of statistical learning states that for \emphbinary
  classification problems, any Empirical Risk Minimization (ERM) learning rule has
  close to optimal sample complexity. In this paper we seek for a generic optimal
  learner for \emphmulticlass prediction.  We start by proving a surprising result:
  a generic optimal multiclass learner must be \emphimproper, namely, it must have
  the ability to output hypotheses which do not belong to the hypothesis class, even
  though it knows that all the labels are generated by some hypothesis from the class.
  In particular, no ERM learner is optimal. This brings back the fundamental question
  of “how to learn”? We give a complete answer to this question by giving a new analysis
  of the one-inclusion multiclass learner of Rubinstein et el (2006) showing that
  its sample complexity is essentially optimal. Then, we turn to study the popular
  hypothesis class of generalized linear classifiers. We derive optimal learners that,
  unlike the one-inclusion algorithm, are computationally efficient. Furthermore,
  we show that the sample complexity of these learners is better than the sample complexity
  of the ERM rule, thus settling in negative an open question due to Collins (2005)'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: daniely14b
month: 0
firstpage: 287
lastpage: 316
page: 287-316
sections: 
author:
- given: Amit
  family: Daniely
- given: Shai
  family: Shalev-Shwartz
date: 2014-05-29
address: Barcelona, Spain
publisher: PMLR
container-title: Proceedings of The 27th Conference on Learning Theory
volume: '35'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 5
  - 29
pdf: http://proceedings.mlr.press/v35/daniely14b.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
